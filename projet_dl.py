# -*- coding: utf-8 -*-
"""Projet_DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16D5h59HXukneyhUks5WdTqWBlVYCZ2I3

# JuristAI Maroc

## 1- Les Installations
"""

# Commented out IPython magic to ensure Python compatibility.
# Installation des librairies principales
# %pip install -q langchain langchain-community langchain-huggingface
# %pip install -q chromadb
# %pip install -q sentence-transformers
# %pip install -q pypdf
# %pip install -q gradio  # Pour l'interface finale

# Installation des librairies pour l'optimisation GPU (Critique pour Colab gratuit)
# %pip install -q -U torch transformers
# %pip install -q accelerate
# %pip install -q bitsandbytes

"""### Pour sauvgarder a google drive"""

from google.colab import drive
import os

# Monter le Drive
drive.mount('/content/drive')

# Cr√©er un dossier sp√©cifique pour ton projet s'il n'existe pas d√©j√†
project_folder = "/content/drive/MyDrive/Projet_DL"
os.makedirs(project_folder, exist_ok=True)

print(f"‚úÖ Drive mont√© et dossier pr√™t : {project_folder}")

"""## V√©rification du d√©tection du GPU"""

import torch

# V√©rifier si CUDA (le GPU) est disponible
if torch.cuda.is_available():
    print(f"‚úÖ GPU d√©tect√© : {torch.cuda.get_device_name(0)}")
    print("Tu es pr√™t pour la suite %")
else:
    print("‚ùå Attention : GPU non d√©tect√©. V√©rifie que tu as bien choisi 'T4 GPU' dans les param√®tres d'ex√©cution.")

"""## 2- chargement de data pdf"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q langchain-text-splitters

from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# "." signifie le dossier local de Colab
pdf_folder_path = "/content/drive/MyDrive"

print("üßê Lecture des fichiers PDF d√©pos√©s...")

loader = PyPDFDirectoryLoader(pdf_folder_path)

try:
    documents = loader.load()

    if len(documents) > 0:
        # --- AJOUT : Extraction des noms de fichiers ---
        # On r√©cup√®re le chemin source de chaque page et on extrait uniquement le nom du fichier
        import os
        noms_fichiers = set([os.path.basename(doc.metadata['source']) for doc in documents])

        print(f"‚úÖ R√©ussi % {len(documents)} pages charg√©es au total.")
        print(f"üìÑ Fichiers d√©tect√©s : {', '.join(noms_fichiers)}")
        # -----------------------------------------------

        # D√©coupage pour le RAG
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=150
        )
        splits = text_splitter.split_documents(documents)
        print(f"‚úÇÔ∏è {len(splits)} segments cr√©√©s. Pr√™t pour l'√©tape suivante %")

    else:
        print("‚ùå Aucun PDF d√©tect√©. Assurez-vous d'avoir gliss√© vos fichiers dans le panneau de gauche (ic√¥ne dossier).")
except Exception as e:
    print(f"‚ùå Erreur : {e}")

"""## 3- Cr√©ation de la Base de Donn√©es Vectorielle"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q langchain-huggingface langchain_community

# Commented out IPython magic to ensure Python compatibility.
import os

# 1. Installations de s√©curit√© et Imports
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
#     %pip install -q langchain-huggingface
    from langchain_huggingface import HuggingFaceEmbeddings

from langchain_community.vectorstores import Chroma

from google.colab import drive
drive.mount('/content/drive')

# 2. Configuration du chemin sur Google Drive
# On cr√©e un dossier d√©di√© pour ne pas m√©langer les fichiers
drive_save_path = "/content/drive/MyDrive/Projet_DL/chroma_db_juridique"
os.makedirs(drive_save_path, exist_ok=True)

# 3. Choix du mod√®le d'Embedding (Le traducteur texte -> vecteurs)
# 'paraphrase-multilingual-MiniLM-L12-v2' est excellent pour le fran√ßais et l'arabe juridique
model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

print(f"üß† Chargement du mod√®le d'intelligence s√©mantique sur le GPU T4...")
embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs={'device': 'cuda'} # Force l'utilisation du GPU T4
)

# 4. Cr√©ation et Sauvegarde de la base
print(f"‚è≥ Transformation des lois en vecteurs... (Patientez, sauvegarde sur le Drive)")

# 'splits' doit avoir √©t√© cr√©√© √† l'√©tape pr√©c√©dente (le d√©coupage des PDF)
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory=drive_save_path
)

print(f"‚úÖ Termin√© % Votre base de donn√©es juridique est s√©curis√©e sur le Drive.")
print(f"üìç Emplacement : {drive_save_path}")

"""## 4- Chargement du Mod√®le de Langage (LLM)

### LLM TinyLlama-1.1B
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install -q ctransformers[cuda]
# %pip install -q langchain-community

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U langchain-community langchain ctransformers[cuda]

from langchain_community.llms import CTransformers

config = {
    'max_new_tokens': 256,
    'repetition_penalty': 1.1,
    'temperature': 0.1,
    'context_length': 2048,      # TinyLlama supporte 2048 context
    'gpu_layers': 50
}

print("üì• Chargement de TinyLlama-1.1B (Compatible CTransformers)...")

llm = CTransformers(
    # Ce mod√®le utilise l'architecture "Llama" classique, donc pas de crash
    model="TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
    model_file="tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
    model_type="llama",          # CTransformers g√®re tr√®s bien ce type
    config=config
)

print("‚úÖ TinyLlama charg√© avec succ√®s !")

"""## 5- Cr√©ation de l'Interface de Chat

"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install gradio
# %pip install chromadb

"""## TinyLlama-1.1B Interface"""

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# 1. Charger la fonction d'embedding (Indispensable pour lire la base)
# Assurez-vous que c'est le m√™me mod√®le que celui utilis√© pour cr√©er la base
print("üì• Chargement des embeddings...")
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# 2. Connexion √† votre base Chroma existante
persist_directory = "/content/drive/MyDrive/Projet_DL/chroma_db_juridique"

vectordb = Chroma(
    persist_directory=persist_directory,
    embedding_function=embeddings
)

print(f"‚úÖ Base de donn√©es charg√©e depuis : {persist_directory}")
print(f"   Nombre de documents trouv√©s : {vectordb._collection.count()}")

import gradio as gr
import re

def generate_response_with_rag(message, history):
    # --- 0. D√©tection d'intention : Salutations (Intent: Greetings) ---
    # Liste de mots-cl√©s pour les salutations (Fran√ßais & Arabe)
    greetings_keywords = [
        "bonjour", "salut", "bonsoir", "coucou", "hi", "hello",
        "salam", "marhaba", "ahlan", "salan", "bon jour"
    ]

    # Nettoyage simple du message pour la v√©rification
    message_clean = message.lower().strip()

    # Si le message est une simple salutation (exacte ou contenue au d√©but)
    # On r√©pond directement sans interroger la base de donn√©es (√©conomie de ressources)
    if any(keyword in message_clean for keyword in greetings_keywords) and len(message_clean) < 20:
        return "Bonjour ! Je suis votre assistant juridique. Posez-moi une question sur vos documents. / ŸÖÿ±ÿ≠bÿß! ÿ£ŸÜÿß ŸÖÿ≥ÿßÿπÿØŸÉ ÿßŸÑŸÇÿßŸÜŸàŸÜŸä. ÿ™ŸÅÿ∂ŸÑ ÿ®ÿ∑ÿ±ÿ≠ ÿ≥ÿ§ÿßŸÑŸÉ."

    # --- 1. Recherche dans la base de donn√©es (Retrieval) ---
    # On cherche les 3 morceaux de texte les plus pertinents
    try:
        docs = vectordb.similarity_search(message, k=3)
        # On compile le contenu trouv√©
        context_text = "\n\n".join([doc.page_content for doc in docs])
    except Exception as e:
        return f"Erreur lors de la recherche : {str(e)}"

    # --- 2. Construction du Prompt (Prompt Engineering) ---
    system_prompt = (
        "Tu es un assistant juridique concis. "
        "R√©ponds dans la m√™me langue que la question (Fran√ßais ou Arabe). "
        "Sois extr√™mement bref et direct (maximum 2 phrases). "
        "Utilise UNIQUEMENT le contexte fourni. "
        "Si la r√©ponse n'est pas dans le contexte, dis 'Je ne sais pas' ou 'ŸÑÿß ÿ£ÿπŸÑŸÖ'."
    )

    formatted_prompt = f"""{system_prompt}

CONTEXTE :
{context_text}

QUESTION : {message}

R√âPONSE :"""

    # --- 3. G√©n√©ration de la r√©ponse ---
    response = llm.invoke(formatted_prompt)

    if hasattr(response, 'content'):
        return response.content
    elif isinstance(response, str):
        return response
    else:
        return str(response)

# Cr√©ation de l'interface
demo = gr.ChatInterface(
    fn=generate_response_with_rag,
    title="‚öñÔ∏è Assistant Juridique (Connect√© Drive)",
    description="Posez une question, je chercherai la r√©ponse dans vos documents.",
    examples=["Bonjour", "Quelles sont les conditions du contrat ?", "D√©lai de pr√©avis ?"]
)

demo.launch(share=True, debug=True)

"""## Mistral 7B Interface"""

!pip install -q -U torch transformers accelerate bitsandbytes langchain langchain-community langchain-huggingface chromadb sentence-transformers gradio

import torch
import gradio as gr
from google.colab import drive
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from langchain_community.vectorstores import Chroma
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig

# -----------------------------------------------------------------------------
# 1. MONTAGE DU DRIVE
# -----------------------------------------------------------------------------
# Si le drive n'est pas d√©j√† mont√©
if not torch.cuda.is_available():
    print("‚ö†Ô∏è ATTENTION : Vous n'utilisez pas le GPU. Activez-le dans 'Modifier le type d'ex√©cution'.")

try:
    drive.mount('/content/drive')
except:
    print("Drive d√©j√† mont√© ou erreur de montage.")

# -----------------------------------------------------------------------------
# 2. CHARGEMENT DE VOTRE BASE DE DONN√âES (CHROMA)
# -----------------------------------------------------------------------------
print("\nüì• Chargement de la base de donn√©es...")
PERSIST_DIRECTORY = "/content/drive/MyDrive/Projet_DL/chroma_db_juridique"

# IMPORTANT : On utilise le m√™me mod√®le d'embedding que lors de la cr√©ation
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

vectordb = Chroma(
    persist_directory=PERSIST_DIRECTORY,
    embedding_function=embedding_model
)
print(f"‚úÖ Base charg√©e. {vectordb._collection.count()} documents disponibles.")

# -----------------------------------------------------------------------------
# 3. CHARGEMENT DE MISTRAL 7B (Optimis√© 4-bit)
# -----------------------------------------------------------------------------
print("\nüöÄ Chargement de Mistral 7B (Cela prend 2-3 minutes)...")

model_id = "mistralai/Mistral-7B-Instruct-v0.2"

# Configuration pour faire tenir le mod√®le sur le GPU gratuit (T4)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto"
)

# Pipeline de g√©n√©ration
text_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,       # Longueur max de la r√©ponse
    temperature=0.1,          # Faible cr√©ativit√© pour rester factuel (juridique)
    repetition_penalty=1.1,
    return_full_text=False
)

llm = HuggingFacePipeline(pipeline=text_pipeline)
print("‚úÖ Mistral est pr√™t !")

# -----------------------------------------------------------------------------
# 4. LOGIQUE DU CHATBOT (RAG + Prompt Mistral)
# -----------------------------------------------------------------------------
def chat_logic(message, history):
    # A. Filtre Salutations (pour r√©pondre vite sans chercher dans la base)
    salutations = ["bonjour", "salut", "bonsoir", "salam", "hi", "hello"]
    if message.lower().strip() in salutations:
        return "Bonjour ! Je suis votre assistant juridique IA. Posez-moi une question sur vos documents."

    # B. Recherche dans la base (Retrieval)
    try:
        docs = vectordb.similarity_search(message, k=3) # On prend les 3 meilleurs extraits
        if not docs:
            return "D√©sol√©, je n'ai trouv√© aucune information pertinente dans vos documents pour r√©pondre √† cette question."

        # Concat√©nation du contexte
        context_text = "\n\n".join([doc.page_content for doc in docs])

        # Extraction des sources (pour affichage)
        sources_list = sorted(list(set([doc.metadata.get('source', 'Inconnu').split('/')[-1] for doc in docs])))
        sources_display = "\n".join([f"- {s}" for s in sources_list])

    except Exception as e:
        return f"Erreur de recherche : {e}"

    # C. Construction du Prompt (Format Mistral Instruct)
    # Les balises [INST] sont cruciales pour que Mistral comprenne que c'est une instruction
    prompt_template = f"""<s>[INST] Tu es un assistant juridique expert. Utilise le contexte ci-dessous pour r√©pondre √† la question de l'utilisateur.
Si la r√©ponse ne se trouve pas dans le contexte, dis simplement "Je ne sais pas".
R√©ponds dans la m√™me langue que la question.

CONTEXTE :
{context_text}

QUESTION :
{message} [/INST]"""

    # D. G√©n√©ration de la r√©ponse
    try:
        response = llm.invoke(prompt_template)
        response_text = response if isinstance(response, str) else response.content

        # Ajout des sources √† la fin
        final_output = f"{response_text}\n\n----------------\nüìö Sources :\n{sources_display}"
        return final_output

    except Exception as e:
        return f"Erreur de g√©n√©ration : {e}"

# -----------------------------------------------------------------------------
# 5. LANCEMENT DE L'INTERFACE GRADIO
# -----------------------------------------------------------------------------
demo = gr.ChatInterface(
    fn=chat_logic,
    title="ü§ñ Assistant Juridique (Mistral 7B + RAG)",
    description="Posez vos questions juridiques. Je cherche la r√©ponse dans votre base ChromaDB.",
    examples=["Quelles sont les conditions de validit√© ?", "Quel est le d√©lai de pr√©avis ?", "Que dit l'article 4 ?"],
    theme="soft"
)

print("\nüåê Lancement de l'interface...")
demo.launch(share=True, debug=True)

"""##Comparaison (TinyLlama-1.1B - Mistral 7B )
### Temps (s)	Vitesse (tok/s)	VRAM Max (GB)  
"""

# 1. INSTALLATION DES D√âPENDANCES
# On installe les biblioth√®ques n√©cessaires pour g√©rer les mod√®les et la quantification
!pip install -q -U torch transformers accelerate bitsandbytes sentencepiece

import torch
import time
import pandas as pd
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
import gc

# ---------------------------------------------------------
# 2. CONFIGURATION
# ---------------------------------------------------------

# Configuration pour charger Mistral en 4-bits (√©conomie de VRAM)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# Fonction pour charger un mod√®le, le tester, puis le d√©charger pour lib√©rer la m√©moire
def evaluer_modele(model_id, nom_modele, context, question, use_4bit=False):
    print(f"\n--- Chargement de {nom_modele} ---")

    # Gestion de la config 4-bit si n√©cessaire
    quant_config = bnb_config if use_4bit else None

    # Chargement du Tokenizer et du Mod√®le
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        quantization_config=quant_config,
        device_map="auto",
        torch_dtype=torch.float16 if not use_4bit else None
    )

    # Pr√©paration du prompt format RAG
    # Note: On force un formatage clair pour aider TinyLlama
    prompt_rag = f"""Vous √™tes un assistant juridique expert. Utilisez le contexte suivant pour r√©pondre √† la question.

    CONTEXTE JURIDIQUE:
    {context}

    QUESTION:
    {question}

    R√âPONSE:"""

    inputs = tokenizer(prompt_rag, return_tensors="pt").to("cuda")

    # Mesure de performance
    torch.cuda.reset_peak_memory_stats()
    start_time = time.time()

    # G√©n√©ration
    outputs = model.generate(
        **inputs,
        max_new_tokens=250, # Limite la longueur pour comparer √©quitablement
        do_sample=True,
        temperature=0.1 # Temp√©rature basse pour √™tre factuel
    )

    end_time = time.time()

    # Calcul des m√©triques
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    reponse_seule = generated_text.replace(prompt_rag, "").strip()

    total_time = end_time - start_time
    num_tokens = len(outputs[0]) - len(inputs['input_ids'][0]) # Tokens g√©n√©r√©s seulement
    tokens_per_sec = num_tokens / total_time
    max_vram = torch.cuda.max_memory_allocated() / 1024**3 # En GB

    print(f"‚úÖ Termin√© en {total_time:.2f}s")

    # Nettoyage M√©moire (CRUCIAL sur Colab)
    del model
    del tokenizer
    torch.cuda.empty_cache()
    gc.collect()

    return {
        "Mod√®le": nom_modele,
        "Temps (s)": round(total_time, 2),
        "Vitesse (tok/s)": round(tokens_per_sec, 2),
        "VRAM Max (GB)": round(max_vram, 2),
        "R√©ponse": reponse_seule
    }

# ---------------------------------------------------------
# 3. DONN√âES DE TEST (Simulez votre contexte juridique ici)
# ---------------------------------------------------------

contexte_juridique = """
Selon l'article 45 du code du travail marocain, l'employeur est tenu de d√©livrer au salari√© un certificat de travail √† la fin du contrat, quel que soit le motif de la rupture.
Ce certificat doit indiquer exclusivement la date d'entr√©e du salari√©, la date de sa sortie et les postes occup√©s.
Toute mention d√©favorable au salari√© est interdite.
"""

question_test = "Que doit contenir le certificat de travail selon la loi et quelles mentions sont interdites ?"

# ---------------------------------------------------------
# 4. EX√âCUTION DU TEST COMPARATIF
# ---------------------------------------------------------

resultats = []

# Test 1: TinyLlama 1.1B (Pas besoin de 4bit, il est petit)
res_tiny = evaluer_modele(
    "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "TinyLlama-1.1B",
    contexte_juridique,
    question_test,
    use_4bit=False
)
resultats.append(res_tiny)

# Test 2: Mistral 7B (Version Instruct v0.2) - Avec 4bit pour tenir sur Colab
res_mistral = evaluer_modele(
    "mistralai/Mistral-7B-Instruct-v0.2",
    "Mistral-7B",
    contexte_juridique,
    question_test,
    use_4bit=False
)
resultats.append(res_mistral)

# ---------------------------------------------------------
# 5. AFFICHAGE DU TABLEAU
# ---------------------------------------------------------
df = pd.DataFrame(resultats)
print("\n\n=== R√âSULTATS DE LA COMPARAISON ===")
display(df[["Mod√®le", "Temps (s)", "Vitesse (tok/s)", "VRAM Max (GB)"]])

print("\n--- R√©ponse TinyLlama ---")
print(df.iloc[0]['R√©ponse'])

print("\n--- R√©ponse Mistral ---")
print(df.iloc[1]['R√©ponse'])

"""### comparaison F1 score et Accuracy"""

print("‚è≥ Installation des d√©pendances...")
!pip install -q -U torch transformers accelerate bitsandbytes sentencepiece matplotlib pandas
!pip install -q -U langchain langchain-community langchain-huggingface chromadb sentence-transformers

# ==========================================
# 1. INSTALLATION & SETUP
# ==========================================
print("‚è≥ Installation des d√©pendances...")
!pip install -q -U torch transformers accelerate bitsandbytes sentencepiece matplotlib "pandas==2.2.2"
!pip install -q -U langchain langchain-community langchain-huggingface chromadb sentence-transformers
!pip install -q -U rouge-score scikit-learn

import torch
import time
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import gc
import re
import string
from collections import Counter

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from google.colab import drive

# Pour les m√©triques avanc√©es
from sklearn.metrics.pairwise import cosine_similarity
from rouge_score import rouge_scorer

# Monter le Drive
print("üìÇ Connexion au Google Drive...")
drive.mount('/content/drive')

# ==========================================
# 2. CHARGEMENT DE LA BASE VECTORIELLE
# ==========================================

# Mod√®le d'embedding (doit √™tre le m√™me que celui utilis√© pour cr√©er la DB)
NOM_MODELE_EMBEDDING = "sentence-transformers/all-MiniLM-L6-v2"
embeddings = HuggingFaceEmbeddings(model_name=NOM_MODELE_EMBEDDING)

# ‚ö†Ô∏è MODIFIEZ CE CHEMIN VERS VOTRE DOSSIER CHROMA
chemin_db = "/content/drive/MyDrive/Projet_DL/chroma_db_juridique"

print(f"üóÑÔ∏è Chargement de la ChromaDB depuis : {chemin_db}")
try:
    db = Chroma(persist_directory=chemin_db, embedding_function=embeddings)
    nb_docs = db._collection.count()
    print(f"‚úÖ Base charg√©e. Documents index√©s : {nb_docs}")
    if nb_docs == 0:
        print("‚ö†Ô∏è ATTENTION : La base est vide !")
except Exception as e:
    print(f"‚ùå Erreur DB (V√©rifiez le chemin) : {e}")
    # Cr√©ation d'une DB vide temporaire pour √©viter que le code plante
    db = Chroma.from_texts(["Document test vide"], embeddings)
    print("‚ö†Ô∏è Base temporaire cr√©√©e. R√©sultats non fiables.")

def recuperer_contexte(question, k=3):
    """R√©cup√®re les documents les plus proches de la question."""
    try:
        docs = db.similarity_search(question, k=k)
        if not docs:
            print(f"‚ö†Ô∏è Aucun document trouv√© pour: {question[:50]}...")
            return "Contexte juridique non disponible."
        contexte = "\n\n".join([doc.page_content for doc in docs])
        return contexte
    except Exception as e:
        print(f"‚ùå Erreur r√©cup√©ration contexte: {e}")
        return "Erreur de r√©cup√©ration du contexte."

# ==========================================
# 3. FONCTIONS DE M√âTRIQUES AVANC√âES (4 TYPES)
# ==========================================

# Initialisation du calculateur Rouge
rouge_calc = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)

def normalize_text(s):
    """Nettoyage : minuscules, suppression articles/ponctuation."""
    if not s:
        return ""
    def remove_articles(text):
        return re.sub(r'\b(le|la|les|un|une|de|des|l|d)\b', ' ', text)
    def white_space_fix(text):
        return ' '.join(text.split())
    def remove_punc(text):
        return ''.join(ch for ch in text if ch not in set(string.punctuation))
    return white_space_fix(remove_articles(remove_punc(s.lower())))

def compute_advanced_metrics(prediction, truth, embedding_model):
    """
    Calcule 4 m√©triques :
    1. Exact Match (0/1)
    2. F1 Token (Recouvrement lexical)
    3. Rouge-L (Structure de phrase)
    4. S√©mantique (Cosine similarity des embeddings)
    """
    pred_norm = normalize_text(prediction)
    truth_norm = normalize_text(truth)

    # 1. Exact Match
    exact_match = 1.0 if pred_norm == truth_norm else 0.0

    # 2. F1 Score (Lexical)
    pred_tokens = pred_norm.split()
    truth_tokens = truth_norm.split()

    if not pred_tokens or not truth_tokens:
        f1 = 1.0 if pred_tokens == truth_tokens else 0.0
    else:
        common = Counter(pred_tokens) & Counter(truth_tokens)
        num_same = sum(common.values())
        if num_same == 0:
            f1 = 0.0
        else:
            precision = 1.0 * num_same / len(pred_tokens)
            recall = 1.0 * num_same / len(truth_tokens)
            f1 = (2 * precision * recall) / (precision + recall)

    # 3. ROUGE-L (Structurelle)
    try:
        scores_rouge = rouge_calc.score(truth_norm, pred_norm)
        rouge_l = scores_rouge['rougeL'].fmeasure
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur ROUGE: {e}")
        rouge_l = 0.0

    # 4. S√©mantique (Cosine Similarity)
    try:
        # Vectorisation des deux phrases
        vec_pred = embedding_model.embed_query(prediction)
        vec_truth = embedding_model.embed_query(truth)
        # Calcul cosine (reshape n√©cessaire pour sklearn)
        similarity = cosine_similarity([vec_pred], [vec_truth])[0][0]
        similarity = max(0.0, similarity)  # √âviter les valeurs n√©gatives
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur embedding metrics : {e}")
        similarity = 0.0

    return {
        "Exact_Match": round(exact_match, 2),
        "F1_Token": round(f1, 4),
        "Rouge_L": round(rouge_l, 4),
        "Semantique": round(similarity, 4)
    }

def evaluer_pertinence_contexte(question, contexte):
    """Mesure la pertinence du contexte r√©cup√©r√©."""
    mots_q = set(normalize_text(question).split())
    mots_c = set(normalize_text(contexte).split())
    if not mots_q:
        return 0.0
    overlap = len(mots_q & mots_c) / len(mots_q)
    return round(overlap, 3)

# ==========================================
# 4. MOTEUR D'√âVALUATION
# ==========================================

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

def evaluer_modele_rag(model, tokenizer, nom_modele, question, ground_truth, embedding_model):
    """√âvalue un mod√®le sur une question avec RAG."""

    # RAG : R√©cup√©ration du contexte
    contexte = recuperer_contexte(question, k=3)
    pertinence = evaluer_pertinence_contexte(question, contexte)

    # Construction du prompt
    prompt = f"""Contexte juridique:
{contexte}

Question: {question}

R√©ponse courte et pr√©cise:"""

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to("cuda")

    # G√©n√©ration
    start_time = time.time()
    try:
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=False,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
        duration = time.time() - start_time

        full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Extraction de la r√©ponse (on retire le prompt)
        reponse = full_text[len(prompt):].strip()
        if '\n\n' in reponse:
            reponse = reponse.split('\n\n')[0].strip()
        elif '\n' in reponse:
            reponse = reponse.split('\n')[0].strip()

        # Limite de longueur
        reponse = reponse[:300] if len(reponse) > 300 else reponse

    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration pour {nom_modele}: {e}")
        reponse = "[Erreur de g√©n√©ration]"
        duration = 0.0
        pertinence = 0.0

    # Calcul des m√©triques
    metrics = compute_advanced_metrics(reponse, ground_truth, embedding_model)

    print(f"   üìù {nom_modele} -> F1: {metrics['F1_Token']:.3f} | Sem: {metrics['Semantique']:.3f} | Pert: {pertinence:.3f}")

    return {
        "Mod√®le": nom_modele,
        "Question": question,
        "V√©rit√© Terrain": ground_truth,
        "R√©ponse G√©n√©r√©e": reponse,
        "Exact Match": metrics['Exact_Match'],
        "F1 Token": metrics['F1_Token'],
        "Rouge-L": metrics['Rouge_L'],
        "S√©mantique": metrics['Semantique'],
        "Pertinence Ctx": pertinence,
        "Temps (s)": round(duration, 2)
    }

# ==========================================
# 5. DATASET DE TEST
# ==========================================

# Dataset de test √©tendu (ajoutez plus d'exemples pour des r√©sultats fiables)
dataset_test = [

    {

        "question": "Quelle est la dur√©e l√©gale de travail au Maroc ?",

        "verite": "44 heures par semaine"

    },

    {

        "question": "Que doit contenir le certificat de travail ?",

        "verite": "La date d'entr√©e, la date de sortie et les postes occup√©s"

    },

    {

        "question": "Quelle est la dur√©e du pr√©avis de d√©mission ?",

        "verite": "8 jours pour moins d'un an d'anciennet√©"

    },

    {

        "question": "Quel est l'√¢ge minimum pour travailler au Maroc ?",

        "verite": "15 ans"

    },

    {

        "question": "Combien de jours de cong√© annuel pay√© ?",

        "verite": "1.5 jours par mois de travail effectif"

    }

]

print(f"\nüìä Dataset de test : {len(dataset_test)} questions")

# ==========================================
# 6. LANCEMENT DE L'√âVALUATION
# ==========================================

resultats_globaux = []

# Liste des mod√®les √† tester (ajoutez use_4bit=True pour √©conomiser la RAM)
modeles = [
    ("TinyLlama/TinyLlama-1.1B-Chat-v1.0", "TinyLlama", False),
    ("mistralai/Mistral-7B-Instruct-v0.2", "Mistral-7B", False),  # ‚úÖ 4-bit activ√©
]

print("\n" + "="*60)
print("üöÄ D√âBUT DE L'√âVALUATION")
print("="*60)

# Boucle principale
for model_id, nom, use_4bit in modeles:
    print(f"\n{'='*60}")
    print(f"üì¶ Chargement de {nom}...")
    print(f"   Quantification 4-bit: {'‚úÖ OUI' if use_4bit else '‚ùå NON'}")
    print(f"{'='*60}")

    try:
        tok = AutoTokenizer.from_pretrained(model_id)

        # Ajout du pad_token si n√©cessaire
        if tok.pad_token is None:
            tok.pad_token = tok.eos_token

        mod = AutoModelForCausalLM.from_pretrained(
            model_id,
            quantization_config=bnb_config if use_4bit else None,
            device_map="auto",
            torch_dtype=torch.float16 if not use_4bit else None,
            low_cpu_mem_usage=True
        )

        print(f"‚úÖ {nom} charg√© avec succ√®s")
        print(f"   M√©moire GPU utilis√©e: {torch.cuda.memory_allocated()/1e9:.2f} GB")

        # √âvaluation sur chaque question
        for i, item in enumerate(dataset_test, 1):
            print(f"\n‚ùì Question {i}/{len(dataset_test)}: {item['question'][:60]}...")
            res = evaluer_modele_rag(
                mod, tok, nom,
                item["question"],
                item["verite"],
                embeddings
            )
            resultats_globaux.append(res)

        # Nettoyage m√©moire GPU entre chaque mod√®le
        print(f"\nüßπ Nettoyage m√©moire pour {nom}...")
        del mod, tok
        torch.cuda.empty_cache()
        gc.collect()
        print(f"   M√©moire GPU lib√©r√©e: {torch.cuda.memory_allocated()/1e9:.2f} GB")

    except Exception as e:
        print(f"\n‚ùå ERREUR CRITIQUE sur {nom}:")
        print(f"   {type(e).__name__}: {e}")
        print("   Le mod√®le sera ignor√©.")
        continue

# ==========================================
# 7. AFFICHAGE DES R√âSULTATS
# ==========================================

print("\n" + "="*60)
print("üìä R√âSULTATS FINAUX")
print("="*60)

if resultats_globaux:
    df = pd.DataFrame(resultats_globaux)

    # Tableau d√©taill√©
    print("\n=== TABLEAU D√âTAILL√â ===")
    display(df[["Mod√®le", "Question", "R√©ponse G√©n√©r√©e", "F1 Token", "Rouge-L", "S√©mantique", "Pertinence Ctx"]])

    # Statistiques moyennes par mod√®le
    print("\n=== MOYENNES PAR MOD√àLE ===")
    metriques_cols = ["Exact Match", "F1 Token", "Rouge-L", "S√©mantique", "Pertinence Ctx", "Temps (s)"]
    df_mean = df.groupby("Mod√®le")[metriques_cols].mean()
    display(df_mean)

    # Sauvegarde des r√©sultats
    chemin_csv = "/content/drive/MyDrive/Projet_DL/resultats_rag_evaluation.csv"
    try:
        df.to_csv(chemin_csv, index=False, encoding='utf-8')
        print(f"\nüíæ R√©sultats sauvegard√©s: {chemin_csv}")
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur sauvegarde CSV: {e}")

    # ==========================================
    # 8. VISUALISATIONS
    # ==========================================

    print("\nüìà G√©n√©ration des graphiques...")

    # Graphique 1 : Comparaison Multi-M√©trique
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    # Barres group√©es
    metriques_viz = ["F1 Token", "Rouge-L", "S√©mantique", "Pertinence Ctx"]
    df_viz = df.groupby("Mod√®le")[metriques_viz].mean()

    ax1 = axes[0]
    df_viz.plot(kind='bar', ax=ax1, rot=0, width=0.8, colormap='viridis')
    ax1.set_title("Performance RAG : Comparaison Multi-M√©trique", fontsize=14, fontweight='bold')
    ax1.set_ylabel("Score (0-1)", fontsize=12)
    ax1.set_ylim(0, 1.15)
    ax1.legend(bbox_to_anchor=(1.0, 1.0), loc='upper left', title="M√©triques")
    ax1.grid(axis='y', linestyle='--', alpha=0.5)

    # Ajout des valeurs sur les barres
    for container in ax1.containers:
        ax1.bar_label(container, fmt='%.3f', padding=3, fontsize=9)

    # Graphique 2 : Temps d'inf√©rence
    ax2 = axes[1]
    temps_moy = df.groupby("Mod√®le")["Temps (s)"].mean()
    bars = ax2.bar(temps_moy.index, temps_moy.values, color=['#3498db', '#e74c3c'], width=0.6)
    ax2.set_title("Temps Moyen d'Inf√©rence", fontsize=14, fontweight='bold')
    ax2.set_ylabel("Temps (secondes)", fontsize=12)
    ax2.grid(axis='y', linestyle='--', alpha=0.5)

    # Valeurs sur les barres
    for bar in bars:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f}s', ha='center', va='bottom', fontsize=11, fontweight='bold')

    plt.tight_layout()
    plt.savefig('/content/drive/MyDrive/Projet_DL/graphique_rag_evaluation.png', dpi=300, bbox_inches='tight')
    print("üíæ Graphique sauvegard√©: graphique_rag_evaluation.png")
    plt.show()

    # Graphique 3 : Heatmap des performances
    plt.figure(figsize=(10, 6))
    pivot_data = df.pivot_table(
        values='F1 Token',
        index='Question',
        columns='Mod√®le',
        aggfunc='mean'
    )

    import seaborn as sns
    sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='RdYlGn', center=0.5,
                cbar_kws={'label': 'Score F1'}, linewidths=0.5)
    plt.title("Heatmap : Performance par Question", fontsize=14, fontweight='bold')
    plt.xlabel("Mod√®le", fontsize=12)
    plt.ylabel("Question", fontsize=12)
    plt.xticks(rotation=0)
    plt.yticks(rotation=0, fontsize=8)
    plt.tight_layout()
    plt.savefig('/content/drive/MyDrive/Projet_DL/heatmap_rag_evaluation.png', dpi=300, bbox_inches='tight')
    print("üíæ Heatmap sauvegard√©e: heatmap_rag_evaluation.png")
    plt.show()

    print("\n‚úÖ √âVALUATION TERMIN√âE AVEC SUCC√àS !")

else:
    print("‚ùå Aucun r√©sultat g√©n√©r√©. V√©rifiez les erreurs ci-dessus.")

print("\n" + "="*60)
print("üèÅ FIN DU SCRIPT")
print("="*60)